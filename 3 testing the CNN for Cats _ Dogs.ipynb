{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dropout,BatchNormalization\n",
    "\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),input_shape=(50,50,1),kernel_initializer = 'orthogonal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Conv2D(32,(3,3),kernel_initializer = 'orthogonal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "\n",
    "model.add(Flatten())\n",
    "#Flatten layer to stack the output convolutions from second convolution layer\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "#Dense layer of 64 neurons\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "#The Final layer with two outputs for two categories\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-050.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
      "[[0.86332166 0.13667831]] 0 Cat 0.86332166\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "category_dict={0:'Cat',1:'Dog'}\n",
    "\n",
    "img_path=r'C:\\Users\\Acer\\Desktop\\Python\\Ai with Thakshila\\archive\\test_set\\test_set\\cats\\cat.4018.jpg'\n",
    "img=cv2.imread(img_path)\n",
    "\n",
    "test_img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "test_img=cv2.resize(test_img,(50,50))\n",
    "test_img=test_img/255\n",
    "test_img=test_img.reshape(1,50,50,1)\n",
    "\n",
    "result=model.predict(test_img)\n",
    "label=np.argmax(result,axis=1)[0]\n",
    "acc=np.max(result,axis=1)[0]\n",
    "\n",
    "print(result,label,category_dict[label],acc)\n",
    "\n",
    "cv2.imshow('LIVE',img)\n",
    "k=cv2.waitKey(0)\n",
    "    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "test_path=r'C:\\Users\\Acer\\Desktop\\Python\\Ai with Thakshila\\archive\\test_imgs'\n",
    "\n",
    "img_names=os.listdir(test_path)\n",
    "#print(img_names)\n",
    "\n",
    "category_dict={0:'Cat',1:'Dog'}\n",
    "\n",
    "for img_name in img_names:\n",
    "    \n",
    "    img=cv2.imread(os.path.join(test_path,img_name))\n",
    "    h,w=img.shape[0:2]\n",
    "\n",
    "    #preprocessing (As done for the training images)\n",
    "    test_img=cv2.resize(img,(50,50))\n",
    "    test_img=cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    test_img=test_img/255\n",
    "    test_img=test_img.reshape(1,50,50,1)\n",
    "    \n",
    "    results=model.predict(test_img)\n",
    "    #print(results)\n",
    "    \n",
    "    label=np.argmax(results,axis=1)[0]\n",
    "    acc=int(np.max(results,axis=1)[0]*100)\n",
    "    \n",
    "    #print(np.argmax(results,axis=1)[0])\n",
    "    #print(results,label,category_dict[label],acc)\n",
    "    \n",
    "    category=category_dict[label]\n",
    "    \n",
    "    if(acc<80):\n",
    "        \n",
    "        category='NONE'\n",
    "    \n",
    "    cv2.putText(img,category,(20,40),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,255,255),2)\n",
    "    cv2.putText(img,str(acc)+'%',(20,80),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,255,255),2)\n",
    "    \n",
    "    cv2.imshow('LIVE',img)\n",
    "    k=cv2.waitKey(0)\n",
    "    if(k==27):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m     ret,img\u001b[38;5;241m=\u001b[39mvideo\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 9\u001b[0m     h,w\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#preprocessing (As done for the training images)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     test_img\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mresize(img,(\u001b[38;5;241m50\u001b[39m,\u001b[38;5;241m50\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "video=cv2.VideoCapture(1)\n",
    "\n",
    "category_dict={0:'Cat',1:'Dog'}\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret,img=video.read()\n",
    "    \n",
    "    h,w=img.shape[0:2]\n",
    "\n",
    "    #preprocessing (As done for the training images)\n",
    "    test_img=cv2.resize(img,(50,50))\n",
    "    test_img=cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)\n",
    "    test_img=test_img/255\n",
    "    test_img=test_img.reshape(1,50,50,1)\n",
    "    \n",
    "    results=model.predict(test_img)\n",
    "    label=np.argmax(results,axis=1)[0]\n",
    "    acc=int(np.max(results,axis=1)[0]*100)\n",
    "    \n",
    "    category=category_dict[label]\n",
    "    \n",
    "    print(results,label,category,acc)\n",
    "    \n",
    "    if(acc<70):\n",
    "        \n",
    "        category='NONE'\n",
    "        \n",
    "    img[0:50,0:w]=[0,255,0]\n",
    "    cv2.putText(img,category,(20,40),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,255,255),2)\n",
    "    cv2.putText(img,str(acc)+'%',(120,40),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,255,255),2)\n",
    "    \n",
    "    cv2.imshow('LIVE',img)\n",
    "    k=cv2.waitKey(100)\n",
    "    if(k==27):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
